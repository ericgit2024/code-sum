# Configuration for Source Code Summarization Project

# Dataset Configuration
dataset:
  name: "code_x_glue_ct_code_to_text"
  language: "python"
  sample_size: 5000  # Scaled to 5000 for improved performance
  train_split: 0.7   # 3500 train
  val_split: 0.15    # 750 val
  test_split: 0.15   # 750 test
  cache_dir: "./data/cache"
  quality_filter_enabled: true  # Enable quality filtering
  
# Quality Filter Configuration
quality_filter:
  min_code_length: 20        # Minimum code characters
  max_code_length: 2000      # Maximum code characters
  min_summary_length: 10     # Minimum summary characters
  max_summary_length: 500    # Maximum summary characters
  min_summary_words: 3       # Minimum words in summary
  max_summary_words: 100     # Maximum words in summary
  min_code_lines: 2          # Minimum code lines
  max_code_lines: 100        # Maximum code lines
  
# Model Configuration
model:
  name: "google/gemma-2b"
  quantization: "4bit"
  load_in_4bit: true
  bnb_4bit_compute_dtype: "float16"
  bnb_4bit_quant_type: "nf4"
  bnb_4bit_use_double_quant: true
  
# LoRA Configuration
lora:
  r: 16
  lora_alpha: 32
  target_modules:
    - "q_proj"
    - "k_proj"
    - "v_proj"
    - "o_proj"
    - "gate_proj"
    - "up_proj"
    - "down_proj"
  lora_dropout: 0.05
  bias: "none"
  task_type: "CAUSAL_LM"

# Training Configuration
training:
  output_dir: "./outputs"
  num_epochs: 3
  per_device_train_batch_size: 2
  per_device_eval_batch_size: 2
  gradient_accumulation_steps: 4
  learning_rate: 2.0e-4
  weight_decay: 0.01
  warmup_steps: 350  # Increased for 5000 samples (~10% of training steps)
  logging_steps: 10
  save_steps: 500    # Adjusted for larger dataset size
  eval_steps: 500
  save_total_limit: 2
  fp16: true
  optim: "paged_adamw_8bit"
  max_grad_norm: 0.3
  max_seq_length: 512
  
# RAG Configuration
rag:
  enabled: false  # DISABLED - Phase 1 showed contamination issue (BLEU dropped from 0.185 to 0.047)
  embedding_model: "microsoft/codebert-base"
  vector_store: "faiss"
  top_k: 2
  chunk_size: 512
  embedding_dim: 768
  index_path: "./data/rag_index"
  
# Structure Extraction Configuration
structure:
  extract_ast: false  # Disabled - too verbose
  extract_cfg: false  # Disabled - too verbose
  extract_pdg: false  # Disabled - too verbose
  use_compact_summary: true  # Use compact summarizer
  max_ast_depth: 10
  max_cfg_nodes: 50
  max_pdg_nodes: 50
  
# Reflective Agent Configuration - ENHANCED with Scoring
reflective_agent:
  enabled: true  # Re-enabled with fixes
  max_iterations: 3  # For training/inference (base value, adjusted by complexity)
  max_iterations_eval: 3  # For evaluation (increased for better quality)
  criteria:
    - "accuracy"      # Does it match what the code does?
    - "completeness"  # Covers main points?
    - "naturalness"   # Plain English, no code syntax?
    - "conciseness"   # Clear and concise?
  threshold_score: 0.7  # Lowered from 0.8 for easier approval
  temperature: 0.7
  # Fast mode settings for evaluation
  fast_mode: false  # Enable for faster evaluation
  greedy_decoding: false  # Use greedy instead of sampling (30-50% faster)
  max_tokens_critique: 250  # Moderate limit with structured prompt
  max_tokens_refinement: 300  # Enough for complete docstrings
  
  # Multi-Criteria Scoring System (Phase 2 - DISABLED for Phase 1 baseline)
  scoring:
    enabled: false  # DISABLED - Phase 1 used keyword-based approval
    weights:
      accuracy: 0.35      # Most important - correctness
      completeness: 0.30  # Second - covers all aspects
      naturalness: 0.20   # Third - readable prose
      conciseness: 0.15   # Fourth - not too verbose
    approval_threshold: 0.75    # Minimum weighted score to approve
    early_stop_threshold: 0.90  # Stop if score exceeds this
    min_improvement: 0.05       # Stop if improvement less than this
  
  # Adaptive Iteration Strategy (Phase 2 - DISABLED for Phase 1 baseline)
  adaptive_iterations:
    enabled: false  # DISABLED - Phase 1 used fixed 3 iterations
    complexity_thresholds:
      simple: 1      # Functions with cyclomatic complexity <= 3
      moderate: 2    # Functions with cyclomatic complexity 4-8
      complex: 3     # Functions with cyclomatic complexity > 8

# Entity Verification Configuration (Replaces Reflective Agent)
entity_verification:
  enabled: true  # Enable entity verification for hallucination detection
  
  # Iteration settings
  max_iterations: 3  # For training/inference
  max_iterations_eval: 3  # For evaluation
  
  # Generation settings
  temperature: 0.7
  max_tokens_refinement: 300  # Enough for complete docstrings
  fast_mode: false  # Enable for faster evaluation
  greedy_decoding: false  # Use greedy instead of sampling (30-50% faster)
  
  # Entity verification thresholds
  hallucination_threshold: 0.30  # Maximum acceptable hallucination score (0.0-1.0)
  min_recall: 0.50              # Minimum recall - at least 50% of entities must be mentioned
  min_f1_score: 0.40            # Minimum F1 score for balanced precision/recall
  entity_weights:
    function_names: 1.0      # Critical - function name must match
    parameter_names: 1.0     # Critical - all parameters must be mentioned
    called_functions: 0.7    # Important - called functions should be accurate
    return_types: 0.7        # Important - return type should be mentioned
    variables: 0.3           # Optional - variables are less critical
  require_all_params: true   # Require all parameters to be mentioned in docstring
  allow_synonyms: true       # Allow synonyms like "returns" vs "outputs"
  
# Evaluation Configuration
evaluation:
  metrics:
    - "bleu"
    - "rouge"
    - "meteor"
    - "bertscore"  # Semantic similarity metric
  use_bertscore: true  # Enable/disable BERTScore (requires bert-score package)
  output_dir: "./evaluation_results"
  
# Prompts - IMPROVED for natural language generation
prompts:
  system_prompt: |
    You are a code documentation expert. Generate concise Python docstrings that describe what functions do.
    
  instruction_template: |
    Generate a concise docstring for this function. Write 1-2 sentences maximum.
    
    Function code:
    ```python
    {code}
    ```
    
    {structure_summary}
    
    Docstring (1-2 sentences, describe what it does):
    
  reflective_agent_prompt: |
    Review this docstring against the code. Rate it on each criterion with a score from 0.0 to 1.0.
    
    Code:
    {code}
    
    Docstring:
    {draft_summary}
    
    Rate each criterion (0.0 = poor, 1.0 = excellent):
    
    Accuracy: [score] - Does it correctly describe what the code actually does? No hallucinations?
    Completeness: [score] - Covers parameters (if any), return value (if any), and key logic (if/else, loops)?
    Naturalness: [score] - Written in plain English? No code syntax like if/else, ==, or function calls?
    Conciseness: [score] - Clear and to the point? Not overly verbose or repetitive?
    
    Overall Decision: APPROVED or NOT APPROVED
    If NOT APPROVED, briefly explain what needs improvement (1 sentence).
    
  refinement_prompt: |
    Improve this docstring based on feedback. Write in natural language only.
    
    Code:
    {code}
    
    Current docstring:
    {draft_summary}
    
    Feedback:
    {feedback}
    
    Write improved docstring (2-4 sentences, plain English, address feedback):
