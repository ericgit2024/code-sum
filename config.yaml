# Configuration for Source Code Summarization Project

# Dataset Configuration
dataset:
  name: "code_x_glue_ct_code_to_text"
  language: "python"
  sample_size: 5000  # Scaled to 5000 for improved performance
  train_split: 0.7   # 3500 train
  val_split: 0.15    # 750 val
  test_split: 0.15   # 750 test
  cache_dir: "./data/cache"
  quality_filter_enabled: true  # Enable quality filtering
  
# Quality Filter Configuration
quality_filter:
  min_code_length: 20        # Minimum code characters
  max_code_length: 2000      # Maximum code characters
  min_summary_length: 10     # Minimum summary characters
  max_summary_length: 500    # Maximum summary characters
  min_summary_words: 3       # Minimum words in summary
  max_summary_words: 100     # Maximum words in summary
  min_code_lines: 2          # Minimum code lines
  max_code_lines: 100        # Maximum code lines
  
# Model Configuration
model:
  name: "google/gemma-2b"
  quantization: "4bit"
  load_in_4bit: true
  bnb_4bit_compute_dtype: "float16"
  bnb_4bit_quant_type: "nf4"
  bnb_4bit_use_double_quant: true
  
# LoRA Configuration
lora:
  r: 16
  lora_alpha: 32
  target_modules:
    - "q_proj"
    - "k_proj"
    - "v_proj"
    - "o_proj"
    - "gate_proj"
    - "up_proj"
    - "down_proj"
  lora_dropout: 0.05
  bias: "none"
  task_type: "CAUSAL_LM"

# Training Configuration
training:
  output_dir: "./outputs"
  num_epochs: 3
  per_device_train_batch_size: 2
  per_device_eval_batch_size: 2
  gradient_accumulation_steps: 4
  learning_rate: 2.0e-4
  weight_decay: 0.01
  warmup_steps: 350  # Increased for 5000 samples (~10% of training steps)
  logging_steps: 10
  save_steps: 500    # Adjusted for larger dataset size
  eval_steps: 500
  save_total_limit: 2
  fp16: true
  optim: "paged_adamw_8bit"
  max_grad_norm: 0.3
  max_seq_length: 512
  
# RAG Configuration
rag:
  enabled: false  # DISABLED - Phase 1 showed contamination issue (BLEU dropped from 0.185 to 0.047)
  embedding_model: "microsoft/codebert-base"
  vector_store: "faiss"
  top_k: 2
  chunk_size: 512
  embedding_dim: 768
  index_path: "./data/rag_index"
  
# Structure Extraction Configuration
structure:
  extract_ast: false  # Disabled - too verbose
  extract_cfg: false  # Disabled - too verbose
  extract_pdg: false  # Disabled - too verbose
  use_compact_summary: true  # Use compact summarizer
  max_ast_depth: 10
  max_cfg_nodes: 50
  max_pdg_nodes: 50
  
# Summary Critic Agent Configuration
summary_critic:
  enabled: true
  max_iterations: 3  # Maximum refinement iterations
  temperature: 0.7
  max_tokens_refinement: 300  # Tokens for refinement generation
  
  # Fast mode settings for evaluation
  fast_mode: false
  greedy_decoding: false
  
  # Parameter analysis settings
  ignore_self: true  # Ignore 'self' parameter
  ignore_cls: true   # Ignore 'cls' parameter
  ignore_args: true  # Ignore *args
  ignore_kwargs: true  # Ignore **kwargs
  min_explanation_threshold: 0.8  # Minimum ratio of explained parameters (80%)


# Entity Verification Configuration (Phase 1 - Hallucination Detection)
entity_verification:
  enabled: true  # Enable entity verification for hallucination detection
  hallucination_threshold: 0.30  # Maximum acceptable hallucination score (0.0-1.0)
  entity_weights:
    function_names: 1.0      # Critical - function name must match
    parameter_names: 1.0     # Critical - all parameters must be mentioned
    called_functions: 0.7    # Important - called functions should be accurate
    return_types: 0.7        # Important - return type should be mentioned
    variables: 0.3           # Optional - variables are less critical
  require_all_params: true   # Require all parameters to be mentioned in docstring
  allow_synonyms: true       # Allow synonyms like "returns" vs "outputs"
  
# Evaluation Configuration
evaluation:
  metrics:
    - "bleu"
    - "rouge"
    - "meteor"
    - "bertscore"  # Semantic similarity metric
  use_bertscore: true  # Enable/disable BERTScore (requires bert-score package)
  output_dir: "./evaluation_results"
  
# Prompts - IMPROVED for natural language generation
prompts:
  system_prompt: |
    You are a code documentation expert. Generate concise Python docstrings that describe what functions do.
    
  instruction_template: |
    Describe what this function does in 1-2 sentences:
    
    {code}
    
    {structure_summary}
    
    Description:
    
  reflective_agent_prompt: |
    Review this docstring against the code. Rate it on each criterion with a score from 0.0 to 1.0.
    
    Code:
    {code}
    
    Docstring:
    {draft_summary}
    
    Rate each criterion (0.0 = poor, 1.0 = excellent):
    
    Accuracy: [score] - Does it correctly describe what the code actually does? No hallucinations?
    Completeness: [score] - Covers parameters (if any), return value (if any), and key logic (if/else, loops)?
    Naturalness: [score] - Written in plain English? No code syntax like if/else, ==, or function calls?
    Conciseness: [score] - Clear and to the point? Not overly verbose or repetitive?
    
    Overall Decision: APPROVED or NOT APPROVED
    If NOT APPROVED, briefly explain what needs improvement (1 sentence).
    
  refinement_prompt: |
    Rewrite this description in plain English. Do NOT use docstring syntax.
    
    Code:
    {code}
    
    Current description:
    {draft_summary}
    
    Feedback:
    {feedback}
    
    Write improved description (2-3 sentences, plain English, NO :param, NO :type):
